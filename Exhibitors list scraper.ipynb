{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is get the list of all exhibitors in a business exhibition.\n",
    "I typically use it either:\n",
    " - before attending an exhibition : it allows me to check every startup attending and list only the best one to see on day D\n",
    " - if I can't get to the exhibiton : perform a check of all exhibitors to verify if some could have been interesting to discuss with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# immports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\r\\n<head>\\r\\n<META NAME=\"robots\" CONTENT=\"noindex,nofollow\">\\r\\n<script src=\"/_Incapsula_Resource?SWJIYLWA=5074a744e2e3d891814e9a2dace20bd4,719d34d31c8e3a6e6fffd425f7e032f3\">\\r\\n</script>\\r\\n<body>\\r\\n</body></html>'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables to adjust to each site\n",
    "BASE_URL=\"https://www.bigdataworld.fr/\"\n",
    "LIST_TO_SURF= [ 'https://www.bigdataworld.fr/exhibitors?page=1',\n",
    "               'https://www.bigdataworld.fr/exhibitors?page=2'\n",
    "    \n",
    "]\n",
    "\n",
    "#test classic scraping\n",
    "url= LIST_TO_SURF[0]\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "page = requests.get(url,headers=headers)\n",
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Ok, seems like it will be harder than thought :  **Website is protecting its data by detecting we are not a real person. Even when setting the user-agent variable, it doesn't allow us to access its data **\n",
    " ** we'll need to use selenium to emulate the surf aof a real person **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of selenium agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare path of Chrome driver\n",
    "driver = webdriver.Chrome(executable_path=r\"c:\\Users\\Utilisateur\\ChromeDriver\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "#test web surfing on a web site\n",
    "driver.get(url)  #go to home page\n",
    "time.sleep(5) #wait ?\n",
    "agent = driver.execute_script(\"return navigator.userAgent\") #check user agent\n",
    "print(agent)\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Note : website is well protected: we have to click 'I am nto a robot' Manually and then we could access its data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exhibitors_list(list):\n",
    "    \"\"\"\n",
    "    Scrape list of all exhibitors (urls) \n",
    "\n",
    "    Args:\n",
    "      list (list) : list of pages url to scrape\n",
    "\n",
    "    Returns:\n",
    "      url_list_all (list) : List of startups/exhibitors profile url. Will be used to scrape startups data alter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    url_list_all=[]\n",
    "    for url in LIST_TO_SURF:\n",
    "        #get list of all startup description url\n",
    "        driver.get(url)\n",
    "        html_page = driver.page_source\n",
    "        soup = BeautifulSoup(html_page,\"lxml\")\n",
    "        link_list = soup.find_all('a', class_='m-exhibitors-list__items__item__header__title__link js-librarylink-entry')\n",
    "        \n",
    "        for link in link_list:\n",
    "            url_list_all.append(link.get('href'))\n",
    "    \n",
    "    return url_list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exhibitors_profile(list):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape all companies main profile urls \n",
    "\n",
    "    Args:\n",
    "      list (list) : list of each companies' profile\n",
    "\n",
    "    Returns:\n",
    "      companies (list) : List of all startups/exhibitors profile\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tic= time.time()\n",
    "    companies =[]\n",
    "    for url in list:\n",
    "        try:\n",
    "            profile = {}\n",
    "            driver.get(BASE_URL+url)\n",
    "            time.sleep(1)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, \"lxml\")\n",
    "            profile['name'] = soup.find('h1', class_=\"m-exhibitor-entry__item__header__infos__title\").text\n",
    "            profile['url'] = soup.find('div', class_=\"m-exhibitor-entry__item__body__contacts__additional__button__website\").find('a').get('href')\n",
    "\n",
    "            # TO DO : ADD YOUTUBE CHANNEL\n",
    "            #youtube_channel = soup.find('i', class_=\"fa fab fa-youtube\")\n",
    "            #youtube_url = youtube_channel.find('a').get('href') if youtube_channel else \"\"\n",
    "            #profil['youtube'] = youtube_url\n",
    "\n",
    "            description = soup.find('div', class_=\"m-exhibitor-entry__item__body__description\")\n",
    "            description = description.text if description else \"\"\n",
    "            profile['description'] = description\n",
    "\n",
    "            companies.append(profile)\n",
    "        except: continue\n",
    "    tac= time.time()\n",
    "    print(\"Total time to scrape: {}\".format(tac-tic))\n",
    "    return companies \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape and save Data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to scrape: 362.2736783027649\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45 entries, 0 to 44\n",
      "Data columns (total 3 columns):\n",
      "description    45 non-null object\n",
      "name           45 non-null object\n",
      "url            45 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.4+ KB\n"
     ]
    }
   ],
   "source": [
    "exhibitors_list = get_exhibitors_list(LIST_TO_SURF)\n",
    "exhibitors_profile_list = get_exhibitors_profile(exhibitors_list)\n",
    "\n",
    "#Import all data into a Dataframe\n",
    "df = pd.DataFrame(exhibitors_profile_list, index=[i for i in range(len(exhibitors_profile_list))])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",description,name,url\n",
      "0,,4IN DATA,https://www.4indata.com/\n",
      "1,\"\n",
      "AEKIDEN est un cabinet de conseil spÃ©cialisÃ© en Data Governance et Data Management, architecte et rÃ©alisateur de la transformation data driven des entreprises et organisations de tous secteurs.\n",
      "Nos offres reflÃ¨tent les diffÃ©rentes facettes (culture, organisation, gouvernance, architecture) qui permettent Ã  nos clients de maÃ®triser leur donnÃ©es et dâ€™en tirer rapidement de la valeur stratÃ©gique et opÃ©rationnelle, avec une claire vision du pourquoi et du quoi, et une grande expÃ©rience du comment.\n",
      "\",Aekiden,http://www.aekiden.com\n",
      "2,\"\n",
      "AMASAI est un cabinet de conseil en Intelligence Artificielle et Data Science.Â \n",
      "Les projets Data et IA sont complexes, et les entreprises manquent de ressources et de compÃ©tences en interne pour les piloter. Les prestataires de service en Data Science et Machine Learning disposent de profils techniques trÃ¨s pointus, mais qui maÃ®trisent mal le business de leurs clients. Cette situation est rendue encore plus ardue par la montÃ©e en puissance des contraintes rÃ©glementaires, et les attentes lÃ©gitimes des tiers sur la maÃ®trise des risques Ã©thiques.Â \n",
      "AMASAI se propose de faire le liant entre le business, la technique et le juridique avec des consultants formÃ©s sur ces trois mÃ©tiers.\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "df.to_csv(\"exhibitors_list.csv\")\n",
    "!head exhibitors_list.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Here we are ! We've got a csv file that we can consult and check all url that match our interest **\n",
    "\n",
    "** For tradeshows where we don't know if our presence is needed, this tool can help guess the potential and interest to show at the exhibition ! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
